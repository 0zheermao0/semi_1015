# LLMæŒ‡å¯¼çš„å›¾ç¥ç»ç½‘ç»œä¸“å®¶æ··åˆæ¨¡å‹ï¼šå®Œæ•´å¤ç°æŒ‡å—

## 1. é—®é¢˜å®šä¹‰ä¸æ ¸å¿ƒåˆ›æ–°

### 1.1 ç ”ç©¶é—®é¢˜
**åŠç›‘ç£å›¾åŸŸé€‚åº” (Semi-supervised Graph Domain Adaptation)**
ç»™å®šä¸€ä¸ªæ ‡æ³¨å……è¶³çš„æºåŸŸå›¾å’Œä¸€ä¸ªæ— æ ‡æ³¨çš„ç›®æ ‡åŸŸå›¾ï¼Œå­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°ç›®æ ‡åŸŸçš„å›¾è¡¨ç¤ºæ¨¡å‹ã€‚

### 1.2 æ ¸å¿ƒåˆ›æ–°ç‚¹

#### ğŸ”¥ **ä¸»è¦è´¡çŒ®ï¼šLLMæŒ‡å¯¼çš„MoEæ¶æ„**
- **ä¸“å®¶é€‰æ‹©çš„è®¤çŸ¥æ™ºèƒ½**ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)åˆ†æå›¾èŠ‚ç‚¹çš„å¤æ‚ç‰¹å¾ï¼Œä¸ºä¸ç¡®å®šæ€§èŠ‚ç‚¹æä¾›ä¸“å®¶é€‰æ‹©å»ºè®®
- **åŒè½¨MoEæ¶æ„**ï¼šæ”¯æŒå¼‚æ„ä¸“å®¶(Original)å’Œç»Ÿä¸€ä¸“å®¶(GAT+Soft-prompt)ä¸¤ç§æ¨¡å¼
- **è‡ªé€‚åº”æŸå¤±å¹³è¡¡**ï¼šåŠ¨æ€è°ƒæ•´å¤šç›®æ ‡æŸå¤±æƒé‡ï¼Œè§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†²çªä¼˜åŒ–é—®é¢˜

#### ğŸ§  **æŠ€æœ¯åˆ›æ–°ç»†èŠ‚**
1. **è®¤çŸ¥å¼•å¯¼æœºåˆ¶**ï¼šå°†å›¾ç»“æ„ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›
2. **ä¸“å®¶ç‰¹åŒ–ç­–ç•¥**ï¼šé€šè¿‡è½¯æç¤ºè®©ç»Ÿä¸€æ¶æ„çš„ä¸“å®¶å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºåå¥½
3. **ä¸ç¡®å®šæ€§æ„ŸçŸ¥é‡‡æ ·**ï¼šåŸºäºä¸“å®¶åˆ†æ­§åº¦æ™ºèƒ½é€‰æ‹©éœ€è¦LLMæŒ‡å¯¼çš„èŠ‚ç‚¹

## 2. æ•°å­¦å»ºæ¨¡ä¸ç†è®ºåŸºç¡€

### 2.1 é—®é¢˜å½¢å¼åŒ–

è®¾ï¼š
- æºåŸŸå›¾ $G_s = (V_s, E_s, X_s, Y_s)$ï¼Œå…¶ä¸­ $|V_s| = n_s$
- ç›®æ ‡åŸŸå›¾ $G_t = (V_t, E_t, X_t)$ï¼Œå…¶ä¸­ $|V_t| = n_t$ï¼Œæ— æ ‡æ³¨
- ç‰¹å¾ç»´åº¦ï¼š$X \in \mathbb{R}^{n \times d}$
- æ ‡ç­¾ç©ºé—´ï¼š$Y \in \{1, 2, ..., C\}$

ç›®æ ‡ï¼šå­¦ä¹ å‡½æ•° $f: \mathbb{R}^d \rightarrow \mathbb{R}^C$ï¼Œæœ€å°åŒ–ç›®æ ‡åŸŸæŸå¤±

### 2.2 LLMæŒ‡å¯¼çš„ä¸“å®¶é€‰æ‹©æ¨¡å‹

#### ä¸“å®¶ç½‘ç»œå®šä¹‰
$$\text{Expert}_i(x, A) = \text{GNN}_i(x, A; \theta_i), \quad i = 1, ..., N$$

#### LLMåå¥½ç”Ÿæˆ
å¯¹äºä¸ç¡®å®šæ€§èŠ‚ç‚¹ $v$ï¼Œæ„å»ºæç¤º $P(v)$ï¼š
```
èŠ‚ç‚¹ç‰¹å¾: [å½’ä¸€åŒ–çš„èŠ‚ç‚¹ç‰¹å¾å‘é‡]
é‚»å±…ç»“æ„: [hop-1é‚»å±…çš„ç»Ÿè®¡ä¿¡æ¯]
ä¸“å®¶è¾“å‡º: [å„ä¸“å®¶çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ]
ä»»åŠ¡: åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºè¯¥èŠ‚ç‚¹æ’åºä¸“å®¶ï¼Œæ ¼å¼ï¼š{"ranking": [0,2,1,3], "confidence": 0.85}
```

#### é€‰æ‹©æŸå¤±å‡½æ•° (DPOç‰ˆæœ¬)
$$\mathcal{L}_{\text{select}} = -\frac{1}{|U|}\sum_{v \in U} \sum_{i,j \in \mathcal{P}_v} \log \sigma(\alpha (s_i^v - s_j^v))$$

å…¶ä¸­ï¼š
- $U$ï¼šä¸ç¡®å®šæ€§èŠ‚ç‚¹é›†åˆ
- $\mathcal{P}_v$ï¼šLLMç”Ÿæˆçš„åå¥½å¯¹
- $s_i^v$ï¼šé—¨æ§ç½‘ç»œå¯¹ä¸“å®¶içš„å¾—åˆ†
- $\alpha$ï¼šæ¸©åº¦å‚æ•°

### 2.3 ä¸€è‡´æ€§æŸå¤±
$$\mathcal{L}_{\text{consistency}} = \frac{1}{|U|}\sum_{v \in U} \sum_{(i,j) \in \mathcal{R}_v} \text{KL}(P_i^v \| P_j^v)$$

å…¶ä¸­ $P_i^v$ æ˜¯ä¸“å®¶iåœ¨èŠ‚ç‚¹vçš„é¢„æµ‹åˆ†å¸ƒï¼Œ$\mathcal{R}_v$ æ˜¯æ’åå¯¹

### 2.4 å¤šæ ·æ€§æŸå¤± (MMDæ­£åˆ™åŒ–)
$$\mathcal{L}_{\text{diversity}} = -\frac{2}{N(N-1)}\sum_{i<j} \text{MMD}^2(\mathcal{D}_i, \mathcal{D}_j) + \text{MMD}^2(\mathcal{C}_i, \mathcal{C}_j)$$

å…¶ä¸­ $\mathcal{D}_i$ å’Œ $\mathcal{C}_i$ åˆ†åˆ«æ˜¯åˆ†å‘çŸ©é˜µå’Œç»„åˆçŸ©é˜µ

### 2.5 æ€»æŸå¤±å‡½æ•°
$$\mathcal{L}_{\text{total}} = w_{\text{cls}} \mathcal{L}_{\text{cls}} + w_{\text{select}} \mathcal{L}_{\text{select}} + w_{\text{consistency}} \mathcal{L}_{\text{consistency}} + w_{\text{diversity}} \mathcal{L}_{\text{diversity}} + w_{\text{gate}} \mathcal{L}_{\text{gate}}$$

## 3. ç½‘ç»œæ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„å›¾
```
è¾“å…¥: (X, A)
    â†“
[ç‰¹å¾ç¼–ç å™¨]
    â†“
[ä¸“å®¶æ··åˆå±‚] â† LLMæŒ‡å¯¼ â† [ä¸ç¡®å®šæ€§é‡‡æ ·]
    â”œâ”€â”€ ä¸“å®¶1: GNN/hop=1 æˆ– GAT+prompt1
    â”œâ”€â”€ ä¸“å®¶2: GNN/hop=2 æˆ– GAT+prompt2
    â”œâ”€â”€ ...
    â””â”€â”€ ä¸“å®¶N: GNN/hop=N æˆ– GAT+promptN
    â†“
[é—¨æ§æœºåˆ¶] â† [å­¦ä¹ åˆ°çš„æƒé‡]
    â†“
[ç‰¹å¾èåˆ]
    â†“
[åˆ†ç±»å™¨] â†’ è¾“å‡ºé¢„æµ‹
```

### 3.2 åŒMoEæ¶æ„å®ç°

#### æ¶æ„1: Original MoE (å¼‚æ„ä¸“å®¶)
```python
class MoE(nn.Module):
    def __init__(self, input_size, output_size, num_experts=6,
                 gnn_type='ppmi', gnn_mode='power_adj'):
        # ä¸“å®¶é…ç½®ï¼šä¸åŒGNNç±»å‹æˆ–ä¸åŒè·³æ•°
        self.experts = [
            PPMIConv(input_size, hidden_dim),  # hop=1
            PPMIConv(input_size, hidden_dim),  # hop=2
            PPMIConv(input_size, hidden_dim),  # hop=3
            GCNConv(input_size, hidden_dim),   # å¤šå±‚1
            GCNConv(input_size, hidden_dim),   # å¤šå±‚2
            SAGEConv(input_size, hidden_dim),  # å¤šå±‚3
        ]
```

#### æ¶æ„2: GAT+Soft-prompt MoE (ç»Ÿä¸€ä¸“å®¶)
```python
class GATSoftPromptMoE(nn.Module):
    def __init__(self, input_size, output_size, num_experts=6,
                 prompt_length=8, num_heads=4):
        # ç»Ÿä¸€çš„ä¸‰å±‚GATæ¶æ„
        self.experts = nn.ModuleList([
            UnifiedGATExpert(input_size, hidden_dim, output_size,
                           prompt_length=prompt_length, expert_id=i)
            for i in range(num_experts)
        ])
```

#### Soft-promptå®ç°
```python
class SoftPrompt(nn.Module):
    def __init__(self, prompt_length, hidden_dim, expert_id):
        # å¯å­¦ä¹ çš„promptå‘é‡
        self.prompt_embeddings = nn.Parameter(
            torch.randn(prompt_length, hidden_dim) * 0.1
        )
        # ä¸“å®¶ç‰¹åŒ–å˜æ¢
        self.specialization_transform = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x):
        # èåˆpromptä¿¡æ¯åˆ°èŠ‚ç‚¹ç‰¹å¾
        prompt_vector = self.prompt_embeddings.mean(dim=0)
        specialized = self.specialization_transform(x)
        fusion_weight = torch.sigmoid(self.fusion_weight)
        return (1-fusion_weight) * specialized + fusion_weight * prompt_vector
```

### 3.3 é—¨æ§æœºåˆ¶è®¾è®¡

#### åŒæ¨¡å¼é—¨æ§
```python
class GateRouter(nn.Module):
    def __init__(self, input_dim, num_experts, gate_mode='embedding'):
        if gate_mode == 'embedding':
            self.gate = nn.Linear(input_dim, num_experts)
        elif gate_mode == 'gat':
            self.gate = GATConv(input_dim, num_experts, heads=1)

    def forward(self, x, edge_index):
        logits = self.gate(x, edge_index)
        return F.softmax(logits, dim=-1)
```

## 4. è®­ç»ƒç­–ç•¥ä¸è¶…å‚æ•°

### 4.1 åŠ¨æ€æŸå¤±è°ƒåº¦
```python
class DynamicLossScheduler:
    def get_cls_weight(self, epoch):
        # åˆ†ç±»æŸå¤±æƒé‡ï¼šå‰æœŸé«˜ï¼ŒåæœŸç¨³å®š
        return 1.0 if epoch < 50 else 0.8

    def get_consistency_weight(self, epoch):
        # ä¸€è‡´æ€§æŸå¤±ï¼šæ¸è¿›å¼å¢åŠ 
        warmup_epochs = int(0.3 * total_epochs)
        if epoch < warmup_epochs:
            return 0.1 + 0.9 * (epoch / warmup_epochs)
        # ä½™å¼¦è°ƒåº¦
        return 0.1 + 0.9 * 0.5 * (1 + cos(Ï€ * (epoch-warmup_epochs) / (total_epochs-warmup_epochs)))

    def get_diversity_weight(self, epoch):
        # å¤šæ ·æ€§æŸå¤±ï¼šå‘¨æœŸæ€§å¢å¼º
        cycle_length = 20
        cycle_progress = (epoch % cycle_length) / cycle_length
        if cycle_progress < 0.5:
            return 0.01 + 0.04 * (2 * cycle_progress)
        else:
            return 0.01 + 0.04 * (2 * (1 - cycle_progress))
```

### 4.2 å…³é”®è¶…å‚æ•°é…ç½®

#### åŸºç¡€é…ç½®
```yaml
# æ¨¡å‹å‚æ•°
input_dim: 50-100          # æ ¹æ®æ•°æ®é›†è°ƒæ•´
output_dim: 512            # ç¼–ç å™¨è¾“å‡ºç»´åº¦
num_experts: 6             # ä¸“å®¶æ•°é‡
hidden_dim: 1024           # åˆ†ç±»å™¨éšè—ç»´åº¦
num_layers: 4              # åˆ†ç±»å™¨å±‚æ•°

# è®­ç»ƒå‚æ•°
learning_rate: 5e-4        # å­¦ä¹ ç‡
weight_decay: 2e-5         # æƒé‡è¡°å‡
dropout: 0.1               # Dropoutç‡
epochs: 200                # è®­ç»ƒè½®æ•°
batch_size: full_graph     # å…¨å›¾è®­ç»ƒ

# LLMå‚æ•°
llm_model: "qwen2.5:7b"   # æˆ–å…¶ä»–å¼€æºLLM
llm_interval: 1            # LLMè°ƒç”¨é—´éš”
uncertainty_k: 100         # ä¸ç¡®å®šæ€§èŠ‚ç‚¹æ•°é‡
max_context_length: 12000  # LLMä¸Šä¸‹æ–‡é•¿åº¦

# æŸå¤±æƒé‡
alpha: 0.61                # DPOæ¸©åº¦å‚æ•°
beta: 0.11                 # ä¸€è‡´æ€§æŸå¤±æƒé‡
div_weight: 0.01           # å¤šæ ·æ€§æŸå¤±æƒé‡
consistency_weight: 1.0    # ä¸€è‡´æ€§æŸå¤±æƒé‡

# ä¼˜åŒ–å™¨é…ç½®
optimizer: "Adam"
lr_scheduler: "adaptive"   # æˆ– "cosine"
lr_patience: 8
lr_factor: 0.6
warmup_epochs: 15
early_stop_patience: 80
```

#### æ•°æ®é›†ç‰¹å®šé…ç½®
```yaml
# dblpv7 â†’ citationv1
source: "dblpv7"
target: "citationv1"
label_rate: 0.05           # 5%æ ‡æ³¨ç‡
gnn_type: "ppmi"           # PPMIå·ç§¯
gate_mode: "embedding"     # é—¨æ§æ¨¡å¼

# acmv9 â†’ citationv1
source: "acmv9"
target: "citationv1"
label_rate: 0.05
gnn_type: "ppmi"
gate_mode: "gat"
```

### 4.3 è®­ç»ƒæµç¨‹ä¼ªä»£ç 

```python
def train_epoch(model, data, optimizer, epoch):
    # 1. å‰å‘ä¼ æ’­
    encoded_output, expert_outputs, gate_loss, clean_logits = model(data.x, data.edge_index)
    logits = classifier(encoded_output)

    # 2. ä¸ç¡®å®šæ€§é‡‡æ ·
    uncertainty_mask = calculate_uncertainty(expert_outputs, classifier, k=uncertainty_k)
    uncertainty_nodes = torch.where(uncertainty_mask)[0].tolist()

    # 3. LLMæŒ‡å¯¼ (æ¯éš”llm_intervalè½®)
    if epoch % llm_interval == 0:
        for node in uncertainty_nodes:
            if node not in cache:
                prompt = build_llm_prompt(node, data, expert_outputs, classifier)
                llm_response = query_llm(prompt)
                ranking, confidence = parse_response(llm_response)
                cache[node] = {"ranking": ranking, "confidence": confidence}

    # 4. è®¡ç®—æŸå¤±
    cls_loss = cross_entropy(logits[label_mask], data.y[label_mask])
    select_loss = dpo_loss(cache, uncertainty_nodes, clean_logits, alpha)
    consistency_loss = kl_consistency_loss(cache, expert_outputs, classifier, beta)
    diversity_loss = mmd_diversity_loss(data.x, expert_outputs, clean_logits)

    # 5. åŠ¨æ€æƒé‡
    w_cls = scheduler.get_cls_weight(epoch)
    w_consistency = scheduler.get_consistency_weight(epoch)
    w_diversity = scheduler.get_diversity_weight(epoch)

    # 6. æ€»æŸå¤±
    total_loss = w_cls * cls_loss + w_consistency * consistency_loss + \
                 w_diversity * diversity_loss + select_loss + gate_loss

    # 7. åå‘ä¼ æ’­
    optimizer.zero_grad()
    total_loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()

    return total_loss.item()
```

## 5. å®ç°ç»†èŠ‚ä¸å·¥ç¨‹æŠ€å·§

### 5.1 å†…å­˜ä¼˜åŒ–ç­–ç•¥

#### å¤§å›¾å¤„ç†
```python
def encode_memory_efficient(model, data):
    """å†…å­˜é«˜æ•ˆç¼–ç ï¼Œé¿å…OOM"""
    try:
        # å°è¯•æ ‡å‡†ç¼–ç 
        return model(data.x, data.edge_index)
    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            # åˆ†æ‰¹å¤„ç†
            batch_size = 1000
            outputs = []
            for i in range(0, data.x.size(0), batch_size):
                batch_data = create_subgraph(data, i, i+batch_size)
                batch_output = model(batch_data.x, batch_data.edge_index)
                outputs.append(batch_output)
            return torch.cat(outputs, dim=0)
```

#### ç¼“å­˜ç®¡ç†
```python
class EnhancedLLMCacheManager:
    def __init__(self, cache_dir="llm_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def load_expert_preferences(self, dataset, config):
        """æ”¯æŒåŠ¨æ€ä¸“å®¶æ•°çš„ç¼“å­˜åŠ è½½"""
        config_hash = self._generate_config_hash(config)
        cache_path = self._get_cache_path(dataset, config_hash)

        if cache_path.exists():
            return torch.load(cache_path)
        else:
            # å°è¯•å…¼å®¹æ€§ç¼“å­˜
            return self._find_compatible_cache(dataset, config['expert_num'])
```

### 5.2 æ•°å€¼ç¨³å®šæ€§æŠ€å·§

#### é—¨æ§ç†µæ­£åˆ™åŒ–
```python
def calculate_gate_entropy_regularization(clean_logits, temperature=1.0):
    """é˜²æ­¢é—¨æ§è¿‡åº¦é›†ä¸­"""
    gate_probs = F.softmax(clean_logits / temperature, dim=-1)
    gate_entropy = -torch.sum(gate_probs * torch.log(gate_probs + 1e-9), dim=-1)
    return -torch.mean(gate_entropy)  # æœ€å¤§åŒ–ç†µ
```

#### PPMIå·ç§¯æ•°å€¼ç¨³å®š
```python
class PPMIConv(nn.Module):
    def forward(self, x, edge_index, edge_attr=None):
        # é¢„å¤„ç†ï¼šè¾¹æƒé‡å½’ä¸€åŒ–
        if edge_attr is None:
            edge_attr = torch.ones(edge_index.size(1), device=x.device)

        # æ•°å€¼ç¨³å®šçš„PPMIè®¡ç®—
        ppmi_matrix = self._compute_ppmi(edge_index, edge_attr, x.size(0))
        ppmi_matrix = torch.clamp(ppmi_matrix, min=1e-8, max=1e8)

        return torch.spmm(ppmi_matrix, x)
```

### 5.3 é”™è¯¯å¤„ç†ä¸é²æ£’æ€§

#### LLMå“åº”è§£æ
```python
def parse_llm_response_robust(response, expert_num):
    """é²æ£’çš„LLMå“åº”è§£æ"""
    # æ–¹æ³•1: ç›´æ¥JSONè§£æ
    try:
        parsed = json.loads(response)
        return validate_ranking(parsed["ranking"], expert_num)
    except:
        pass

    # æ–¹æ³•2: æ­£åˆ™è¡¨è¾¾å¼æå–
    import re
    pattern = r'\[\s*(\d+\s*,\s*)*\d+\s*\]'
    matches = re.findall(pattern, response)
    for match in matches:
        try:
            ranking = [int(x) for x in re.findall(r'\d+', match)]
            if validate_ranking(ranking, expert_num):
                return ranking
        except:
            continue

    # æ–¹æ³•3: éšæœºå›é€€
    return list(range(expert_num))
```

## 6. å®éªŒéªŒè¯æ–¹æ¡ˆ

### 6.1 æ•°æ®é›†å‡†å¤‡

#### å­¦æœ¯ç½‘ç»œæ•°æ®é›†
```python
class DomainData(InMemoryDataset):
    """æ ‡å‡†å­¦æœ¯ç½‘ç»œæ•°æ®é›†åŠ è½½å™¨"""

    def __init__(self, root, name):
        # æ•°æ®é›†ç»Ÿè®¡
        # dblpv7: 19,997èŠ‚ç‚¹, 63,626è¾¹, 4ç±»
        # citationv1: 9,227èŠ‚ç‚¹, 37,735è¾¹, 3ç±»
        # acmv9: 11,464èŠ‚ç‚¹, 54,886è¾¹, 3ç±»

        self.data = torch.load(f"{root}/{name}/processed/data.pt")
        self.num_classes = len(torch.unique(self.data.y))
```

#### æ•°æ®é¢„å¤„ç†
```python
def preprocess_graph(data):
    """å›¾æ•°æ®é¢„å¤„ç†"""
    # 1. ç‰¹å¾å½’ä¸€åŒ–
    data.x = F.normalize(data.x, p=2, dim=1)

    # 2. æ„å»ºPPMIçŸ©é˜µ
    if config.gnn_type == 'ppmi':
        ppmi_edge_index, ppmi_edge_weight = compute_ppmi(data.edge_index, data.num_nodes)
        data.ppmi_edge_index = ppmi_edge_index
        data.ppmi_edge_weight = ppmi_edge_weight

    # 3. ç”Ÿæˆè®­ç»ƒæ©ç 
    label_mask = create_label_mask(data.y.size(0), label_rate=0.05)
    data.train_mask = label_mask

    return data
```

### 6.2 è¯„ä¼°æŒ‡æ ‡

#### ä¸»è¦æŒ‡æ ‡
```python
def evaluate_model(model, data):
    """æ¨¡å‹è¯„ä¼°"""
    model.eval()
    with torch.no_grad():
        output = model(data.x, data.edge_index)
        pred = output.argmax(dim=1)

        # ç›®æ ‡åŸŸå‡†ç¡®ç‡
        target_mask = data.test_mask  # å‡è®¾ç›®æ ‡åŸŸ
        accuracy = (pred[target_mask] == data.y[target_mask]).float().mean()

        # F1åˆ†æ•°
        macro_f1 = f1_score(data.y[target_mask].cpu(),
                          pred[target_mask].cpu(), average='macro')
        micro_f1 = f1_score(data.y[target_mask].cpu(),
                          pred[target_mask].cpu(), average='micro')

    return accuracy, macro_f1, micro_f1
```

### 6.3 æ¶ˆèå®éªŒè®¾è®¡

#### å®éªŒçŸ©é˜µ
```python
ablation_configs = {
    # LLMæœºåˆ¶æ¶ˆè
    "full_method": {"llm_interval": 1},
    "no_llm_random": {"llm_interval": -1, "expert_selection": "random"},
    "no_llm_uniform": {"llm_interval": -1, "expert_selection": "uniform"},
    "llm_static": {"llm_interval": 999},

    # æ¶æ„æ¶ˆè
    "original_moe": {"moe_architecture": "original"},
    "gat_soft_prompt": {"moe_architecture": "gat_soft_prompt"},
    "single_expert": {"expert_num": 1},

    # æŸå¤±æ¶ˆè
    "no_consistency": {"consistency_weight": 0.0},
    "no_diversity": {"div_weight": 0.0},
    "no_dpo": {"alpha": 0.0},

    # GNNç±»å‹æ¶ˆè
    "ppmi_conv": {"gnn_type": "ppmi"},
    "gcn_conv": {"gnn_type": "gcn"},
    "sage_conv": {"gnn_type": "sage"},
}
```

### 6.4 åŸºçº¿å¯¹æ¯”

#### ç«äº‰æ–¹æ³•
1. **æºåŸŸè®­ç»ƒ**: ä»…åœ¨æºåŸŸè®­ç»ƒï¼Œç›´æ¥æµ‹è¯•ç›®æ ‡åŸŸ
2. **CaNet**: å¯¹æŠ—åŸŸé€‚åº”ç½‘ç»œ
3. **LDAT**: é•¿å°¾åŸŸé€‚åº” transformer
4. **MARIO**: å¤šé‡è¡¨ç¤ºå­¦ä¹ å’Œå¯¹é½
5. **SGDA**: åŠç›‘ç£å›¾åŸŸé€‚åº”

#### å®éªŒè®¾ç½®
```yaml
# å¯¹æ¯”å®éªŒé…ç½®
datasets: [
    ["dblpv7", "citationv1"],
    ["dblpv7", "acmv9"],
    ["citationv1", "acmv9"]
]

seeds: [1, 2, 3, 42, 100]  # å¤šç§å­å®éªŒ
label_rates: [0.05, 0.1]   # ä¸åŒæ ‡æ³¨ç‡
metrics: ["accuracy", "macro_f1", "micro_f1"]
```

## 7. éƒ¨ç½²ä¸ä¼˜åŒ–

### 7.1 ç¯å¢ƒé…ç½®

#### ä¾èµ–åŒ…
```bash
# æ ¸å¿ƒä¾èµ–
torch>=2.0.0
torch-geometric>=2.3.0
torch-scatter>=2.1.0
torch-sparse>=0.6.0

# LLMç›¸å…³
openai>=1.0.0
transformers>=4.30.0
accelerate>=0.20.0

# å®éªŒå·¥å…·
wandb>=0.15.0
numpy>=1.21.0
scikit-learn>=1.0.0
pandas>=1.3.0
matplotlib>=3.5.0

# å¯é€‰GPUåŠ é€Ÿ
# pip install torch_geometric torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html
```

#### GPUé…ç½®
```python
# å†…å­˜ç›‘æ§
def print_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")

# è‡ªåŠ¨æ··åˆç²¾åº¦
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    output = model(data.x, data.edge_index)
    loss = compute_loss(output, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 7.2 åˆ†å¸ƒå¼è®­ç»ƒ

#### å¤šGPUè®­ç»ƒ
```python
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_distributed(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def train_distributed(rank, world_size):
    setup_distributed(rank, world_size)

    # åŒ…è£…æ¨¡å‹
    model = model.to(rank)
    model = DDP(model, device_ids=[rank])

    # æ•°æ®åŠ è½½å™¨éœ€è¦åˆ†å¸ƒå¼é‡‡æ ·å™¨
    dataset = YourDataset()
    sampler = torch.utils.data.distributed.DistributedSampler(
        dataset, num_replicas=world_size, rank=rank
    )
```

### 7.3 ç”Ÿäº§éƒ¨ç½²

#### æ¨¡å‹ä¿å­˜ä¸åŠ è½½
```python
def save_checkpoint(model, optimizer, epoch, best_acc, filepath):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_accuracy': best_acc,
        'config': config.__dict__,
    }
    torch.save(checkpoint, filepath)

def load_checkpoint(filepath, model, optimizer):
    checkpoint = torch.load(filepath)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch'], checkpoint['best_accuracy']
```

#### æ¨ç†ä¼˜åŒ–
```python
class InferenceModel:
    def __init__(self, model_path):
        self.model = self.load_model(model_path)
        self.model.eval()

    def predict(self, data):
        with torch.no_grad():
            # æ‰¹é‡æ¨ç†
            batch_size = 1000
            predictions = []

            for i in range(0, len(data.x), batch_size):
                batch_x = data.x[i:i+batch_size]
                batch_edge_index = self.extract_subgraph_edges(data.edge_index, i, i+batch_size)

                output = self.model(batch_x, batch_edge_index)
                pred = output.argmax(dim=1)
                predictions.append(pred)

            return torch.cat(predictions, dim=0)
```

## 8. ç»“æœåˆ†æä¸å¯è§†åŒ–

### 8.1 ä¸“å®¶ç‰¹åŒ–åˆ†æ

#### ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
```python
def analyze_expert_specialization(model, data):
    """åˆ†æä¸“å®¶ç‰¹åŒ–æ¨¡å¼"""
    expert_indices, expert_probs = model.get_node_expert_assignment(data.x, data.edge_index)

    # ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ
    expert_usage = torch.zeros(model.num_experts)
    for i in range(model.num_experts):
        expert_usage[i] = (expert_indices == i).float().mean()

    # æŒ‰ç±»åˆ«åˆ†æä¸“å®¶åå¥½
    class_expert_matrix = torch.zeros(data.num_classes, model.num_experts)
    for node_id in range(len(data.y)):
        class_id = data.y[node_id]
        for expert_id in expert_indices[node_id]:
            class_expert_matrix[class_id, expert_id] += 1

    return {
        'expert_usage': expert_usage,
        'class_expert_matrix': class_expert_matrix,
        'entropy': -torch.sum(expert_usage * torch.log(expert_usage + 1e-9))
    }
```

### 8.2 è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–

#### æŸå¤±æ›²çº¿
```python
def plot_training_curves(log_file):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    df = pd.read_csv(log_file)

    fig, axes = plt.subplots(2, 2, figsize=(12, 8))

    # æŸå¤±æ›²çº¿
    axes[0,0].plot(df['epoch'], df['train_loss'], label='Train')
    axes[0,0].plot(df['epoch'], df['val_loss'], label='Val')
    axes[0,0].set_title('Loss Curves')
    axes[0,0].legend()

    # å‡†ç¡®ç‡æ›²çº¿
    axes[0,1].plot(df['epoch'], df['source_acc'], label='Source')
    axes[0,1].plot(df['epoch'], df['target_acc'], label='Target')
    axes[0,1].set_title('Accuracy')
    axes[0,1].legend()

    # ä¸“å®¶å¤šæ ·æ€§
    axes[1,0].plot(df['epoch'], df['diversity_loss'])
    axes[1,0].set_title('Expert Diversity')

    # é—¨æ§ç†µ
    axes[1,1].plot(df['epoch'], df['gate_entropy'])
    axes[1,1].set_title('Gate Entropy')

    plt.tight_layout()
    plt.savefig('training_curves.png')
```

### 8.3 åµŒå…¥ç©ºé—´å¯è§†åŒ–

#### t-SNEå¯è§†åŒ–
```python
def visualize_embeddings(model, data):
    """å¯è§†åŒ–å­¦ä¹ åˆ°çš„åµŒå…¥"""
    model.eval()
    with torch.no_grad():
        embeddings = model.encode(data.x, data.edge_index)

    # t-SNEé™ç»´
    from sklearn.manifold import TSNE
    tsne = TSNE(n_components=2, random_state=42)
    embeddings_2d = tsne.fit_transform(embeddings.cpu().numpy())

    # ç»˜åˆ¶
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                         c=data.y.cpu().numpy(), cmap='tab10', alpha=0.6)
    plt.colorbar(scatter)
    plt.title('t-SNE Visualization of Learned Embeddings')
    plt.xlabel('t-SNE Dimension 1')
    plt.ylabel('t-SNE Dimension 2')
    plt.savefig('embeddings_tsne.png')
```

## 9. å¸¸è§é—®é¢˜ä¸è°ƒè¯•æŒ‡å—

### 9.1 è®­ç»ƒé—®é¢˜

#### æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
```python
# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# å±‚å½’ä¸€åŒ–
self.layer_norm = nn.LayerNorm(hidden_dim)

# æ®‹å·®è¿æ¥
output = x + self.transform(x)
```

#### å†…å­˜ä¸è¶³
```python
# 1. å‡å°æ‰¹æ¬¡å¤§å°
batch_size = 500  # ä»1000å‡å°‘åˆ°500

# 2. æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 3. æ£€æŸ¥ç‚¹æŠ€æœ¯
from torch.utils.checkpoint import checkpoint
output = checkpoint(self.expensive_function, input_tensor)
```

### 9.2 æ€§èƒ½ä¼˜åŒ–

#### è®¡ç®—æ•ˆç‡
```python
# 1. ç¨€ç–çŸ©é˜µä¼˜åŒ–
import torch_sparse
adj = torch.sparse_coo_tensor(edge_index, edge_weight, (n, n))

# 2. JITç¼–è¯‘
@torch.jit.script
def fast_gcn_forward(x, edge_index, weight):
    return torch.spmm(edge_index, x @ weight)

# 3. é¢„è®¡ç®—å’Œç¼“å­˜
@lru_cache(maxsize=128)
def compute_ppmi_matrix(edge_index_tuple, num_nodes):
    # ç¼“å­˜PPMIçŸ©é˜µè®¡ç®—ç»“æœ
    pass
```

### 9.3 è°ƒè¯•æŠ€å·§

#### ä¸­é—´ç»“æœæ£€æŸ¥
```python
def debug_forward(model, data):
    """è°ƒè¯•å‰å‘ä¼ æ’­è¿‡ç¨‹"""
    print(f"Input shape: {data.x.shape}")

    # æ£€æŸ¥æ¯ä¸ªä¸“å®¶çš„è¾“å‡º
    with torch.no_grad():
        for i, expert in enumerate(model.experts):
            expert_out = expert(data.x, data.edge_index)
            print(f"Expert {i} output: mean={expert_out.mean():.4f}, "
                  f"std={expert_out.std():.4f}, "
                  f"nan_count={torch.isnan(expert_out).sum()}")

    # æ£€æŸ¥é—¨æ§æƒé‡
    gate_logits = model.gate_router(data.x)
    gate_probs = F.softmax(gate_logits, dim=-1)
    print(f"Gate probs: mean={gate_probs.mean():.4f}, "
          f"entropy={-torch.sum(gate_probs * torch.log(gate_probs + 1e-9), dim=-1).mean():.4f}")
```

## 10. å¤ç°æ£€æŸ¥æ¸…å•

### 10.1 ä»£ç å®ç°æ£€æŸ¥
- [ ] åŒMoEæ¶æ„æ­£ç¡®å®ç°
- [ ] LLMæç¤ºæ„å»ºé€»è¾‘æ­£ç¡®
- [ ] æŸå¤±å‡½æ•°è®¡ç®—æ— è¯¯
- [ ] åŠ¨æ€æƒé‡è°ƒåº¦æ­£ç¡®
- [ ] ç¼“å­˜æœºåˆ¶å·¥ä½œæ­£å¸¸

### 10.2 å®éªŒé…ç½®æ£€æŸ¥
- [ ] æ•°æ®é›†é¢„å¤„ç†æ­£ç¡®
- [ ] è¶…å‚æ•°è®¾ç½®åˆç†
- [ ] è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ­£ç¡®
- [ ] å¤šç§å­å®éªŒè®¾ç½®
- [ ] åŸºçº¿å¯¹æ¯”å…¬å¹³

### 10.3 æ€§èƒ½åŸºå‡†
```yaml
expected_performance:
  dblpv7_to_citationv1:
    target_accuracy: 0.85-0.90
    macro_f1: 0.80-0.85
    training_time: "1-2 hours on single GPU"

  dblpv7_to_acmv9:
    target_accuracy: 0.80-0.85
    macro_f1: 0.75-0.80
    training_time: "1-2 hours on single GPU"
```

### 10.4 å¯å¤ç°æ€§ä¿è¯
- [ ] éšæœºç§å­è®¾ç½®
- [ ] ç¯å¢ƒä¾èµ–ç‰ˆæœ¬å›ºå®š
- [ ] æ•°æ®åˆ’åˆ†æ–¹å¼ä¸€è‡´
- [ ] æ¨¡å‹åˆå§‹åŒ–ç›¸åŒ
- [ ] è®­ç»ƒæµç¨‹ç¡®å®š

---

## æ€»ç»“

æœ¬å¤ç°æŒ‡å—æä¾›äº†LLMæŒ‡å¯¼çš„å›¾ç¥ç»ç½‘ç»œä¸“å®¶æ··åˆæ¨¡å‹çš„å®Œæ•´æŠ€æœ¯ç»†èŠ‚ï¼ŒåŒ…æ‹¬ï¼š

1. **ç†è®ºåŸºç¡€**ï¼šå®Œæ•´çš„æ•°å­¦å»ºæ¨¡å’Œç®—æ³•æ¨å¯¼
2. **æ¶æ„è®¾è®¡**ï¼šåŒMoEæ¶æ„çš„è¯¦ç»†å®ç°
3. **è®­ç»ƒç­–ç•¥**ï¼šåŠ¨æ€æŸå¤±è°ƒåº¦å’Œä¼˜åŒ–æŠ€å·§
4. **å·¥ç¨‹å®ç°**ï¼šå†…å­˜ä¼˜åŒ–ã€é”™è¯¯å¤„ç†ã€éƒ¨ç½²æ–¹æ¡ˆ
5. **å®éªŒéªŒè¯**ï¼šå…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆå’Œå¯¹æ¯”å®éªŒ
6. **è°ƒè¯•æŒ‡å—**ï¼šå¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

éµå¾ªæœ¬æŒ‡å—ï¼Œç ”ç©¶è€…å¯ä»¥å®Œæ•´å¤ç°è¯¥è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„åˆ›æ–°å’Œæ”¹è¿›ã€‚